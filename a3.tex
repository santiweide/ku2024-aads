\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{upquote}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\newenvironment{statement}[2][Statement]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\usepackage{xcolor}




\title{Assignment 3}


\author{Author \\
  Wanjing Hu / fng685@alumni.ku.dk  \\
  Zhigao Yan / sxd343@alumni.ku.dk  \\
  Wenshuo Dong / gnj461@alumni.ku.dk  \\
  Jiayi Zhang / xrw579@alumni.ku.dk \\
} 
 

\begin{document}
\maketitle
% The ones marked * are the hard, and will get larger weight.

\section{Hashing-2.1}
\textbf{Question: } Is the truly independent hash function h : U $\rightarrow$ [m] universal?
%wanjing

Yes. 

Our reason is that for truly independent hash function, for any input $x_i \in U$, $i = 1,2,...,n$, the outputs $h(x_i)$ are completely independent and uniformly distributed over $[m]={0,1,...,m-1}$. Assume we are picking the value $k$ such that $h(x)=k$ , then we have

\begin{equation}
\begin{aligned}
\mathop{Pr} \limits_{h} [h(x)=k] \leq \frac{1}{m}\\
\mathop{Pr} \limits_{h} [h(y)=k] \leq \frac{1}{m}\\
\end{aligned}
\end{equation}
That is $Pr[h(x)=h(y)] \leq \frac{1}{m}$. So the truly independent hash function h : U $\rightarrow$ [m] is universal.

\section{Hashing-2.2}
\textbf{Question:} If a hash function h : U → [m] has collision probability 0, how large must m be?

The collision probability is 0 which means:
\[\mathop{Pr} \limits_{h}[h(x)=h(y)] =0\]
So, the minimum size of m should be equal to \(|U|\), in other words \(|m|\geq|U|\)
%zhigao

\section{Hashing-2.3}
%wenshuo
\textbf{Question:}\\
Let u $\leq$ m. Is the identity function f(x) = x a universal hash function [u]$\rightarrow$[m]?\\
\textbf{Answer:}\\
A hash function \( h \) is universal if for any distinct \( x, y \in [u] \), the probability of a collision satisfies:
\[
P[h(x) = h(y)] \leq \frac{1}{m}.
\]
Then consider the identity function \( f(x) = x \):
\begin{enumerate}
    \item When \( u \leq m \), each element in \( [u] \) maps to a unique element in \( [m] \) because \( x \) maps directly to itself, with no collisions.
    \item This implies:
    \[
    P[f(x) = f(y)] = 0, \quad \text{for } x \neq y,
    \]
    which satisfies:
    \[
    P[f(x) = f(y)] \leq \frac{1}{m}.
    \]
\end{enumerate}
The identity function \( f(x) = x \) is a universal hash function in this scenario because it satisfies the definition of universality.

\section{Hashing-2.4}
% in (b) assume x is not in S
%jiayi
\textbf{Question:} 
Assume that $x \notin S$ and that $h$ is universal. Let $I(y)$ be an indicator variable which is $1$ if $h(x) = h(y)$ and $0$ otherwise. Then the expected number of elements in $L[h(x)]$ is
\begin{align}
    E_h[|L[h(x)]|] = E_h [\sum_{y\in S} I(y)]  = \sum_{y \in S} E_h[I(y)] = \sum_{y \in S} Pr_h[h(y) = h(x)] \leq n/m \leq 1
\end{align}
 The second equality uses linearity of expectation.
(a) What is the expected number of elements in $L[h(x)]$ if $x \in S$ ?
(b) What bound do you get if $h$ is only 2-approximately universal? (assume $x$ is not in $S$)

\textbf{Answer}

(a) similar to the suppose of the question, assume that $x \in S$, $h$ is universal and $I(y)$ be the indicator.

because of $x \in S$, so $Pr_h[h(y) = h(x)] = 1/m$ when $x \neq y$, and $I(y)= 1$ when $x = y$\\

so $E_h[|L[h(x)]|]$ can be divided into two parts:
\begin{equation}
\begin{aligned}
    &E_h[|L[h(x)]|] \\
    &= E_h[I(x)]+ E_h [\sum_{y\in S, x \neq y} I(y)]  \\
    &= 1 + \sum_{y \in S, x \neq y} E_h[I(y)]\\
    &\leq 1 + (n - 1)/m
\end{aligned}
\end{equation}

So the expected number of elements in $L[h(x)]$ if $x \in S$ is $1 + (n - 1) / m$.

(b) if $h$ is only 2-approximately universal, 
\begin{align}
Pr_h[h(y) = h(x)] \leq 2/m
\end{align}
and the $E_h[|L[h(x)]|] $ will be:
\begin{align}
    E_h[|L[h(x)]|] = \sum_{y \in S} Pr_h[h(y) = h(x)] \leq 2 * n / m
\end{align}
\section{Hashing-2.5*}
%wanjing
\textbf{Question: }
With $s: U \rightarrow [n^3]$ and $h : U \rightarrow [n]$ independent universal hash functions, for a given $x \in U \backslash S$, what is the probability of a false positive when we search $x$, that is, what is the probability that there is a key $y \in S$ such that $h(y) = h(x)$ and $s(y) = s(x)$?

\textbf{Answer}

Now we have:
\begin{equation}
\begin{aligned}
\mathop{Pr} \limits_{h} [h(x)=h(y)] &= \frac{1}{n}\\
\mathop{Pr} \limits_{s} [s(x)=s(y)] &= \frac{1}{n^3}
\end{aligned}
\end{equation}

And we want to know about:
\begin{equation}
\begin{aligned}
&\mathop{Pr}  [False \, Positive]\\
&= Pr[\exists y \in S \, such \, that \, h(y) = h(x) \, and \, s(y) = s(x)]\\
&<= \sum_{y \in S} Pr[h(x)=h(y) \, and \, s(y)=s(x)]
\end{aligned}
\end{equation}

The inequality is a union bound, that the probability of that at least one of multiple events happen is at most the sum of their probabilities(reference to the hashing material provided in class). Since h and s are independent, for any single $y \in S$ we have:
\begin{equation}
\begin{aligned}
&\mathop{Pr} [h(x)=h(y) \, and \, s(y)=s(x)] \\
&\mathop{Pr} [h(x)=h(y)] * \mathop{Pr} \limits_{h,s} [s(x)=s(y)] \\
&= \frac{1}{n^4}
\end{aligned}
\end{equation}

So we have
\begin{equation}
\begin{aligned}
&\mathop{Pr}  [False \, Positive]\\
&<= |S|/n^4
\end{aligned}
\end{equation}

Where $|S|$ is the size of set S.

\section{Hashing-2.6*}
\textbf{Question:}
Suppose we for our hash function also consider a = 0, that is, for random \((a, b)\in[p]^2\), we define the hash function \(h_{a,b} : [p] \rightarrow [m]\) by
\[h_{a,b}(x) = ((ax + b)\mod{p} )\mod{m}\]

(a) Show that this function may not be universal.\\
(b) Prove that it is always 2-approximately universal, that is, for any distinct \(x, y \in [p]\), 
\[Pr_{(a,b)\in[p]^2}[h_{a,b}(x) = h_{a,b}(y)] < 2/m\]


From the previous proof, we know that when \(a \neq 0\):
\[Pr_{(a,b)\in[p]^2}[h_{a,b}(x) = h_{a,b}(y)] < 1/m\]
And the probability of \(a \neq 0\) is \(\frac{p-1}{p}\).
So we can calculate the probability of two cases:
\[
Pr[h_{a,b}(x) = h_{a,b}(y)] = Pr[a = 0] \cdot 1 + Pr[a \neq 0] \cdot \frac{1}{m}.
\]
\[
Pr[h_{a,b}(x) = h_{a,b}(y)] = \frac{1}{p} + \frac{p-1}{p} \cdot \frac{1}{m}.
\]
From the question we can get \(p>m\), Hence:
\[\frac{1}{p} + \frac{p-1}{p} \cdot \frac{1}{m}<\frac{2}{m}\]
Finally, Substitute into the inequality:
\[Pr_{(a,b)\in[p]^2}[h_{a,b}(x) = h_{a,b}(y)] < 2/m\]



%zhigao

\section{Hashing-3.1}
%wenshuo
\textbf{Question:}\\
Generalize 2-independence. What is 3-independence? k-independence?\\
\textbf{Answer:}\\
A hash function \( h : [u] \to [m] \) is \( k \)-independent if for any distinct \( k \) keys \( x_1, x_2, \dots, x_k \in [u] \) and any \( k \) hash values \( q_1, q_2, \dots, q_k \in [m] \), the following holds:
\[
P[h(x_1) = q_1 \land h(x_2) = q_2 \land \dots \land h(x_k) = q_k] = \frac{1}{m^k}.
\]
Each key \( x_i \) is hashed uniformly to a value in \( [m] \). The hash values for any \( k \) distinct keys are independent random variables.
\begin{enumerate}
    \item 2-independence (Strong Universality): For two distinct keys \( x, y \in [u] \), the hash values \( h(x) \) and \( h(y) \) are independent and uniformly distributed:
    \[
    P[h(x) = q \land h(y) = r] = \frac{1}{m^2}.
    \]
    \item 3-independence: For three distinct keys \( x, y, z \in [u] \), the hash values \( h(x), h(y), h(z) \) are independent and uniformly distributed:
    \[
    P[h(x) = q \land h(y) = r \land h(z) = s] = \frac{1}{m^3}.
    \]
\end{enumerate}
A \( k \)-independent hash function ensures that any \( k \) keys are hashed independently, with the joint probability of their hash values being the product of individual probabilities.

\section{Hashing-3.2}
%jiayi
\textbf{Question: }
If $h$ is c-approximately strongly universal, what is an upper bound on the pairwise event probability,

\begin{align}
&Pr[h(x) = q \wedge h(y) = r]?
\end{align}

\textbf{Answer:}
As $h$ is c-approximately strongly universal, 
\begin{align}
    Pr[h(x) = q \wedge h(y) = r] = Pr[h(x) = q] \wedge Pr[h(y) = r]\leq c^2/m^2
\end{align}
So, the upper bound of it is $c^2/m^2$

\section{Hashing-3.3* }
% you must prove that the collision probability is at most c/m. It is not sufficient to prove c²/m
%wanjing TODO
\textbf{Question: }
Argue that if $h: U \rightarrow [m]$ is c-approximately strong universal, then h is also c-approximately universal.

\textbf{Answer}
The collision probability of the c-approximately strong universal is 

\begin{equation}
\begin{aligned}
&Pr[h(x)=h(y)] \\
&= \sum_{q \in [m]} Pr[h(x) = q \, and \, h(y) = q]\\
&= \sum_{q \in [m]} Pr[h(x) = q \, and \, h(y) = h(x)]\\
&=\sum_{q \in [m]} Pr[h(x) = q] * Pr[h(y) = h(x)]\\
&=\sum_{q \in [m]} Pr[h(x) = q] *1/m
\end{aligned}
\end{equation}

For the last equation we use the independency property of c-approximately universal. And we also have:
\begin{equation}
\begin{aligned}
Pr[h(x) = q] &<= \frac{c}{m} \\
\end{aligned}
\end{equation}

So we have
\begin{equation}
\begin{aligned}
&Pr[h(x)=h(y)] \\
&= \sum_{q \in [m]} Pr[h(x) = q] * 1/m\\
&<=\sum_{q \in [m]}   \frac{c}{m^2}
= \frac{c}{m}
\end{aligned}
\end{equation}

\section{Hashing-3.4*}
\textbf{Question:} Is Multiply-Shift c-approximately strongly universal for any constant c?
% hint: the answer is no, because not all pairs of keys hash independently - your job is to prove this.
%zhigao... wanjing done but...not sure.... 

\textbf{Answer}
No. 

For $x=0$ we have 

\begin{equation}
\begin{aligned}
h(0) = \lfloor \frac{a * 0 \mod 2^w}{2^{w-l}} \rfloor = 0
\end{aligned}
\end{equation}

Which means $h(0)$ is always hashed to 0 regardless of the choice of $a$. 

Thus the Multiply-Shift hash function cannot be strongly universal, or even c-approximately strongly universal for any constant c, because:

\begin{equation}
\begin{aligned}
Pr[h(x)=h(y)] \neq \frac{1}{m}
\end{aligned}
\end{equation}

and the deviation cannot be bounded by any constant factor c since $h(0)$ is deterministic.

\section{Hashing-3.5}
%wenshuo
\textbf{Question:}\\ Given the \( S_{h,t}(B) \) and \( S_{h,t}(C) \), how would you estimate the size of the symmetric difference \( (B \setminus C) \cup (C \setminus B) \).\\
\textbf{Answer:}\\
\[
(B \setminus C) \cup (C \setminus B) = (B \cup C) \setminus (B \cap C).
\]
Let \( S_{h,t}(B) \) and \( S_{h,t}(C) \) be the samples obtained using a strongly universal hash function \( h \) with threshold \( t \). From these, we can compute the sampled versions of the union and intersection of \( B \) and \( C \):
    \[
    S_{h,t}(B \cup C) = S_{h,t}(B) \cup S_{h,t}(C).
    \]
    \[
    S_{h,t}(B \cap C) = S_{h,t}(B) \cap S_{h,t}(C).
    \]
The sampled symmetric difference can be computed as:
\[
S_{h,t}((B \setminus C) \cup (C \setminus B)) = S_{h,t}(B \cup C) \setminus S_{h,t}(B \cap C).
\]
To estimate the size of the symmetric difference \( |(B \setminus C) \cup (C \setminus B)| \), we scale the size of the sampled symmetric difference by the ratio \( \frac{m}{t} \), where \( m \) is the range of the hash function and \( t \) is the sampling threshold:
\[
|(B \setminus C) \cup (C \setminus B)| \approx \left| Sh,t(B \cup C) \setminus Sh,t(B \cap C) \right| \cdot \frac{m}{t}.
\]
\section{Hashing-3.6}
%jiayi
\textbf{Question:}
Suppose that $|A| = 100,000,000$ and $p = t/m = 1/100$. Then $E[X] = \mu = 1,000,000$. Give an upper bound for the probability that $|X - \mu| \geq 10,000$. These numbers correspond to a 1\% sampling rate and a 1\% error. 
The bound from (11) is good for predicting range of outcomes, but often what we have is an experiment giving us a concrete value for our random variable $X$, and now we want some confidence interval for the unknown mean $\mu$ that we are trying to estimate.

\textbf{Answer:}
According to Lemma 3.2 which is (11)
\begin{align}
    Pr[|X - \mu| \geq q\sqrt{\mu}] \leq 1 / q^2
\end{align}
And as \,$\mu = 1,000,000$,\\
So 

\begin{equation}
\begin{aligned}
    &Pr[|X - \mu| \geq 10000]\\
    &=  Pr[|X - \mu| \geq 10 *\sqrt{\mu}]\\
    & \leq 1/10^2\\
    &= 1/100
\end{aligned}
\end{equation}
The upper bound is 1/100.

\section{Complexity-34.1-1}
%wanjing 
\textbf{Question}
Define the optimization problem $LONGEST-PATH-LENGTH$ as the relation that associates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices. Define the decision problem $LONGEST-PATH= \{\langle G, u, v, k\rangle: G = (V, E)$ is an undirected graph, $u, v \in V, k \ge 0$ is an integer, and there exists a simple path from $u$ to $v$ in $G$ consisting of at least $k$ edges $\}$. Show that the optimization problem $LONGEST-PATH-LENGTH$ can be solved in polynomial time if and only if $LONGEST-PATH \in P$.

\textbf{Answer}

We split the target into two cases to prove, one is to prove when the $LONGEST-PATH \subseteq P$, the $LONGEST-PATH-LENGTH \subseteq P$(Note as A), and the other is when he $LONGEST-PATH-LENGTH \subseteq P$, the $LONGEST-PATH \subseteq P$(Note as B).
 
\subsection{Proof of A}
Assume the $LONGEST-PATH \subseteq P$, and we apply binary search for finding the longest path length. The steps are as follows:

\textbf{Initialize} Initialize $low = 0$ and $high = |V| - 1$, for the path visits each vertex exactly once, and the longest path in a graph cannot exceed $|V| - 1$ edges . 

\textbf{Step1} Set $mid = \lfloor (low + high) / 2 \rfloor$.

\textbf{Step2} Use the polynomial-time algorithm for $LONGEST-PATH$ to check if there is a path of length at least $mid$ from $u$ to $v$.

\textbf{Step3} If yes, it means there may be a longer path, and we update $low = mid + 1$.

\textbf{Step4} If no, it means the longest path is shorter than current range, and we update $high = mid - 1$.

\textbf{Result} The value of $high$ at the end of the search will give the length of the longest simple path between $u$ and $v$.

Since the decision problem runs in polynomial time, and binary search over $|V|$ iterations requires at most $O(\log |V|)$ queries, the entire process is polynomial in the size of the graph.

\subsection{Proof of B}
Assume the $LONGEST-PATH-LENGTH \subseteq P$, and we have a polynomial time algorithm A that computes the length of the longest simple path between $u$ and $v$ in $G$. We do the following steps:

\textbf{Step1} Input to $LONGEST-PATH$: $G, u, v, k$
\textbf{Step2} Compute $l=A(G,u,v)$, the length f the longest simple path between $u,v$
\textbf{Step3} If $l \ge k$ output YES, means a simple path of at least $k$ edges exists; Otherwise output NO.

\textbf{Complexity}: Since $LONGEST-PATH-LENGTH \subseteq P$, $A \in P$. Also checking $l \ge k$ is in polynomial time.

Thus $LONGEST-PATH \in P$.

\section{Complexity-34.1-5}
%zhigao
\textbf{Question:} Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polynomial time, then it runs in polynomial time. Also show that a polynomial number of
calls to polynomial-time subroutines may result in an exponential-time algorithm.\\
\textbf{Proof:}\\
For a polynomial-time subroutine, If we call it constant number C times, we get:
\[T(n) = O(n^{d_1}) + O(n^{d_1}) + ...+O(n^{d_c}) + O(n^x) \]
We can see, that even if The input is growing, then since the call is constant times, the final result \(d_c\) is still a constant.
Hence,
\[T(n) = O(n^{d_c}) + O(n^x)\]
That is also in polynomial-time.
For the second question, consider every time the algorithm calls the polynomial-time subroutines, we have \(n->n^2\), and we set the algorithm calls the polynomial-time subroutines \(n\) (polynomial number) times.
Hence the total running time of this algorithm is:
\[T(n) = O(n) + O(n^2) + ...+O(n^{2n-1}) + O(n^x) \]
\[T(n) \in O(n^{2^n})\]
That is an exponential-time algorithm.



\section{Complexity-34.2-5}
%wenshuo
\textbf{Question:}\\
Show that any language in NP can be decided by an algorithm running in time $2^{O(n^k)}$ for some constant k.\\
\textbf{Answer:}\\
A language \(L\) is in \(\text{NP}\) if there exists a nondeterministic Turing machine \(N\) and a polynomial \(p(n)\), such that for any input \(x\):
\begin{enumerate}
    \item If \(x \in L\), there exists a computation path in \(N\) that accepts \(x\).
    \item If \(x \notin L\), \(N\) rejects \(x\) on all computation paths.
    \item \(N\)'s runtime on input \(x\) is at most \(p(|x|)\), where \(|x|\) is the length of \(x\).
\end{enumerate}
A nondeterministic Turing machine can be simulated by a deterministic Turing machine through exploration of all possible computation paths. We note the following:
\begin{itemize}
    \item On an input of length \(n\), \(N\) runs for at most \(p(n)\) steps.
    \item At each step, \(N\) has a finite number of possible transitions, say \(c\) (a constant depending on \(N\)'s design).
\end{itemize}
Thus, the total number of computation paths in \(N\) is bounded by:
\[
c^{p(n)}
\]
where \(p(n)\) is the polynomial time bound of \(N\).

To simulate \(N\) deterministically, a deterministic Turing machine \(M\) enumerates all these paths and simulates each one. For each path:
\begin{itemize}
    \item \(M\) simulates \(N\)'s behavior for at most \(p(n)\) steps.
    \item \(M\) checks if any path leads to acceptance (to decide if \(x \in L\)).
\end{itemize}
The runtime of \(M\) is therefore:
\[
\text{(Number of paths)} \times \text{(time per path)} = c^{p(n)} \cdot O(p(n))
\]
Since \(p(n)\) is a polynomial, say \(p(n) = O(n^k)\) for some constant \(k\), the runtime of \(M\) becomes:
\[
c^{p(n)} \cdot O(p(n)) = c^{O(n^k)} \cdot O(n^k)
\]
Using the fact that \(c^{O(n^k)} = 2^{O(n^k) \cdot \log_2(c)}\), and since \(\log_2(c)\) is a constant, the runtime can be simplified as:
\[
2^{O(n^k)}
\]





\section{Complexity-34.2-6}
%jiayi
\textbf{Question:}
A \textbf{hamiltonian path} in a graph is a simple path that visits every vertex exactly once. Show that the language \textbf{HAM-PATH} = \{$<G, u, v>$: there is a hamiltonian path from $u$ to $v$ in graph G\} belongs to \textbf{NP}.

\textbf{Answer:}
Suppose that, there is a HAM-PATH from $u$ to $v$ in graph G and the path is: ($u, v_1, v_2,...,v_n, v$)\\
In the road, there is a edge between every two neighboring vertexes above, and there are $n+2$ vertexes in $G$.\\
To verify the certificate in polynomial time:\\
1 check if $v_i == v$ or $v_i == u$, need to spend a constant time.
2 check whether the $v_i$ exactly appears once, use a set to do this need to spend a constant time too.\\
3 check if there is a edge between $v_{i-1}$ and $v_i$, this is a constant time operation too if we use data structure like a two-dimensional array.\\
So, the \textbf{HAM-PATH} = \{$<G, u, v>$: there is a hamiltonian path from $u$ to $v$ in graph G\} belongs to \textbf{NP}.

\section{Complexity-34.2-8}
%wanjing
\textbf{Question}
Let $\phi$ be a boolean formula constructed from the boolean input variables $x_1, x_2, \dots, x_k$, negations ($\neg$), ANDs ($\vee$), ORs ($\wedge$), and parentheses. The formula $\phi$ is a \textit{\textbf{tautology}} if it evaluates to $1$ for every assignment of $1$ and $0$ to the input variables. Define $TAUTOLOGY$ as the language of boolean formulas that are tautologies. Show that $TAUTOLOGY \in co-NP$.

\textbf{Answer}
Now we have:

 $TAUTOLOGY$=\{$\langle$C$\rangle$ $\mid$ C is a satisfiable $\phi$ formula outputs 1\}
 
 
 $\overline{TAUTOLOGY} $=\{$\langle$C$\rangle$ $\mid$ C is a satisfiable $\phi$ formula outputs 0\}
 
So to verify $TAUTOLOGY \in co-NP$ we need to prove $\overline{TAUTOLOGY} \in NP$. 
 
%We find that the $\phi$ could be transformed into a graph with input variables as the edges, and the negations ($\neg$), ANDs ($\vee$), ORs ($\wedge$) as vertices. Then the evaluation of the $\phi$ could be done in polynomial time as it is the same as finding paths on an acyclic graph from starting point to ending point.
 
%We construct algorithm $A$ with inputs $x$ and $y$.  $A$ checks that $x$ represents a boolean combination in$phi$  with one output result and $y$ represents an assignment of truth values to the result of $\phi$.

%Suppose we create a directed graph $G=(V,E)$ with one vertex for each combinational element and with k directed edges for each vertex whose fan-out is k; the graph contains a directed edge $(u,v)$, if a wire connects the output of element $u$ to an input of element $v$. Then G must be acyclic. Then the evaluation of the $\phi$ could be done in polynomial time as it is the same as finding paths on an acyclic graph from starting point to ending point.

\subsection{Certificate}
The certificate for $\overline{TAUTOLOGY}$ is the assignment $(x_1,x_2,...,x_k) \in {0,1}^k$ such that $\phi(x_1,x_2,...,x_k) = 0$.

\subsection{Verifier}
The verifier is defined as follows:

\textbf{Step1} Substitute each $x_i$ in $\phi$ with the corresponding value in the certificate.

\textbf{Step2} Evaluate based on the negations ($\neg$), ANDs ($\vee$), ORs ($\wedge$), and parentheses, which costs polynomial time in the size of $\phi$.

\textbf{Step3} Check if the result of the evaluation is 0, which would be accepted. Otherwise it will be rejected. 

Since the complexity is polynomial time in the size of $\phi$, $\overline{TAUTOLOGY} \in NP$.

\section{Complexity-34.3-2}
%zhigao
\textbf{Question:} Show that the \(\leq_P\) relation is a transitive relation on languages. That is, show that if \(L_1 \leq_PL_2 \) and \(L_2\leq_P L_3\), then \(L_1\leq_P L_3\). \\
\textbf{Proof:}\\
For \(L_1 \leq_PL_2 \) and \(L_2\leq_P L_3\), we have:
\[x \in L_1 \iff f_1(x) \in L_2\]
\[x \in L_2 \iff f_2(x) \in L_3\]
So we can define a function that is \(f_2(f_1(x))\), hence:
\[x \in L_1 \iff f_2(f_1(x)) \in L_3\]
Now we need to verify this function can be run in polynomial time.
The total runtime for this function is:
\[T_1(n)+T_2(T_1(n))\]
\(T_1(n)\) and \(T_2(n)\) are polynomial-time, so the total runtime is also polynomial-time.
\section{Complexity-34.3-3}
%wenshuo
\textbf{Question:}\\
Prove that \( L \leq_p \overline{L} \) if and only if \( \overline{L} \leq_p L \), where \( \leq_p \).\\
\textbf{Answer:}\\
A language \( A \) is reducible to a language \( B \), written \( A \leq_p B \), if there exists a polynomial-time computable function \( f \) such that for every string \( x \):
   \[
   x \in A \iff f(x) \in B.
   \]
\( \overline{L} \): The complement of a language \( L \), i.e., \( \overline{L} = \Sigma^* \setminus L \), where \( \Sigma^* \) is the set of all strings over the input alphabet.

\subsubsection*{\( \Rightarrow \): Assume \( L \leq_p \overline{L} \)}
- By definition, there exists a polynomial-time computable function \( f \) such that for every string \( x \):
  \[
  x \in L \iff f(x) \in \overline{L}.
  \]
- By the definition of \( \overline{L} \), \( f(x) \in \overline{L} \) means \( f(x) \notin L \). Thus:
  \[
  x \in L \iff f(x) \notin L.
  \]
- Construct a new function \( g \) that maps \( x \) to \( f(x) \), reversing membership:
  \[
  g(x) = f(x).
  \]
  - If \( x \in \overline{L} \), then \( g(x) \in L \) because \( f(x) \notin L \).
  - Therefore, \( \overline{L} \leq_p L \), as \( g(x) \) is computable in polynomial time.

\subsubsection*{\( \Leftarrow \): Assume \( \overline{L} \leq_p L \)}
- By definition, there exists a polynomial-time computable function \( g \) such that for every string \( x \):
  \[
  x \in \overline{L} \iff g(x) \in L.
  \]
- By the definition of \( \overline{L} \), \( x \in \overline{L} \) means \( x \notin L \). Thus:
  \[
  x \notin L \iff g(x) \in L.
  \]
- Equivalently:
  \[
  x \in L \iff g(x) \notin L.
  \]
- Construct a new function \( f \) that maps \( x \) to \( g(x) \), for complement membership:
  \[
  f(x) = g(x).
  \]
  - If \( x \in L \), then \( f(x) \in \overline{L} \) because \( g(x) \notin L \).
  - Therefore, \( L \leq_p \overline{L} \), as \( f(x) \) is computable in polynomial time.
Both directions are proven. Thus:
\[
L \leq_p \overline{L} \iff \overline{L} \leq_p L.
\]

\end{document}
